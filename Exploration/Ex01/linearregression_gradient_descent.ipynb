import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

#1. Bring on the Data😎
data = load_diabetes()
df_X = data.data  # feature
df_y = data.target  # target


#2. Transform to Numpy array (X)
X = np.array(df_X)

#3. Transform to Numpy array (y)
y = np.array(df_y)


#4. Separate to Train and Test Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#5. Define Weights and bias : Initialize the Parameters
np.random.seed(42)
W = np.random.rand(X_train.shape[1]) #Weights
b = 0 #bias
epochs = 3000

#6. Loss Function : Define MSE Function
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 7. Define Gradient Function

def predict(X, W, b):
    return np.dot(X, W) + b


def gradient(X, y, W, b, learning_rate, epochs):
    n = len(y)
    for i in range(epochs):
        # 예측값 계산
        y_pred = predict(X, W, b)

        # MSE 손실 계산
        loss = mean_squared_error(y, y_pred)

        # 경사 계산
        dw = -(2/n) * np.dot(X.T, (y - y_pred))
        db = -(2/n) * np.sum(y - y_pred)

        # 가중치 및 절편 업데이트
        W -= learning_rate * dw
        b -= learning_rate * db

        # 매 100번째 에포크마다 Loss 출력
        if i % 100 == 0:
            print(f"Epoch {i}, Loss: {loss}")

    return W, b

#8. Define Hyperparameter
learning_rate = 0.1

#9.  Learning Model
W, b = gradient_descent(X_train, y_train, W, b, learning_rate, epochs)

#10. Check the Performance about Test data 
y_pred = predict(X_test, W, b)
test_loss = mean_squared_error(y_test, y_pred)
print(f"Test Loss: {test_loss}")

#11. Visualization
plt.scatter(X_test[:, 0], y_test, color='blue', label='Actual Target')
plt.scatter(X_test[:, 0], y_pred, color='red', label='Predicted Target', alpha=0.6)
plt.xlabel("First Feature of X")
plt.ylabel("Target Value")
plt.legend()
plt.title("Actual vs Predicted Target Values")
plt.show()
